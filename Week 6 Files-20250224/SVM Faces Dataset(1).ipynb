{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a39aa9",
   "metadata": {},
   "source": [
    "# Validation Notes\n",
    "\n",
    "For the last few weeks we have split the dataset into 3 sets, training, test and validation as described in lecture 2 slides 21-24. I did it this way to emphasise how the test set should be used. I also wanted you to see something with confusion_matrix and classification_reports which are easier to do this way.\n",
    "\n",
    "However, this is not always the best way of doing things, particularly if we have limited data. A better way of doing things is to use a cross-validation score to make decisions instead.\n",
    "\n",
    "So here's the idea\n",
    "1. Split the data into training and test data (2 sets)\n",
    "2. With the training data, try and come up with a final model\n",
    "3. Score any type of model you build using a cross-validation score, either use kFolds, cross_val_score or GridSearchCV\n",
    "4. Your final model hyperparameters is the one with the highest cross-validation score\n",
    "5. With the final chosen model, evaluate it on the test set (classification_report as well as everything else)\n",
    "\n",
    "With cross_val_score, you don't have to only rely on accuracy, that is just the default, look at the manual https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html or cross_val_score? . You can get different f1 scores\n",
    "\n",
    "| F1 Type     | Best for                          | Handles Imbalance? | Formula Basis              |\n",
    "|------------|----------------------------------|------------------|----------------------------|\n",
    "| **Macro**  | Equal importance to all classes  | ❌ No            | Mean of per-class F1       |\n",
    "| **Weighted** | Reflects dataset distribution  | ✅ Yes           | Weighted mean of per-class F1 |\n",
    "| **Micro**  | Overall accuracy-based evaluation | ✅ Yes           | Global precision & recall  |\n",
    "\n",
    "f1_macro, f1_weighted, f1_micro setting the scoring attribute in cross_val_score. Similar in GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc823416-5d87-4522-88bd-fe6d0d1e93f2",
   "metadata": {},
   "source": [
    "# Face Recognition\n",
    "\n",
    "In this lab we are going to build a facial recognition model using Support Vector Machines. The state of the art for image processing is to use Convolutional Neural Networks, but we'll try SVMs now to see how they do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b98bbe7-4933-4647-a6f2-c9bd07b23307",
   "metadata": {},
   "source": [
    "fetch_lfw_people can get us these images. We will take all the people in the dataset that have at least 60 images of that person. This may take a while to run as it will download the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b4f85ec-71a8-4dce-b385-44ea944b0af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'\n",
      " 'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair']\n",
      "(1348, 125, 94)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=60, resize=1)\n",
    "print(faces.target_names)\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9fd071-8884-4668-ac46-0c3b137233b6",
   "metadata": {},
   "source": [
    "So we have 8 different people to train our model on and 1348 total images. The image resolution is 125x94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f18219a-95eb-4a5f-b21e-9cd3e4ff9c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1348, 11750)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccc4fd1-ef2a-489f-aabb-6294dfe4adff",
   "metadata": {},
   "source": [
    "Why does this have shape 1348, 11750?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d4b80d0-a8fc-4c2c-bc22-8a533ba9d149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11750"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "125*94"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c78946",
   "metadata": {},
   "source": [
    "Let's check how many we have of each person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c419289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images per person\n",
    "unique_people, counts = np.unique(faces.target, return_counts=True)\n",
    "\n",
    "# Get corresponding names\n",
    "person_names = [faces.target_names[i] for i in unique_people]\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(person_names, counts, color='skyblue')\n",
    "plt.xlabel(\"Number of Images\")\n",
    "plt.ylabel(\"Person\")\n",
    "plt.title(\"Histogram of Images per Person in LFW Dataset\")\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc19a0",
   "metadata": {},
   "source": [
    "Is this a balanced set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22038655-e918-47ea-9ba4-f97521c484ac",
   "metadata": {},
   "source": [
    "Let's set up our X and Y. faces.data and faces.target seem like good places to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61d46441-445e-47e0-99a0-806be7df4b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d237bcb-6341-489d-8903-5748df1b02d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5254902 , 0.5176471 , 0.5058824 , ..., 0.00653595, 0.00261438,\n",
       "       0.        ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3368b5f-f936-4b04-a035-bf3a04d6950c",
   "metadata": {},
   "source": [
    "shows the images are greyscale with values between 0 and 1 it looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8c5f25-6252-4301-883a-30862ecbf3b4",
   "metadata": {},
   "source": [
    "Do a train test split as usual, use a random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81610cc7-8b87-4da8-a112-b418ceb21418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a6f8cd5-8b6f-44c7-8553-a5f88b716abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd7d797-be23-43bf-833c-7bfce6239e02",
   "metadata": {},
   "source": [
    "Do a quick test with things we know, build and test a SVM linear model with C=2, a SVM rbf model with C=2, a LogisticRegression model and a kNN neighbours model with n=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4ab98a2-5152-4218-807c-f5af05253830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the model types\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef95ea7-ff2b-46d9-86ad-5d32ed7edd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear', C=2.0)\n",
    "%time cross_val_score(model, X_train, y_train)\n",
    "%time model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f3ef75-79e2-4163-a4ea-6efb25a512ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bfc207-125f-4054-adcb-2d4fcd413604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logisticregression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7990ccb0-3793-4a57-8864-70e6ae4c2e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rbf SVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c74a3d-dc69-48d3-9e02-140257fea259",
   "metadata": {},
   "source": [
    "### My results\n",
    "\n",
    "When I ran it, LogisticRegression got the best test score (but took ages to run, 2 and a half minutes!) , then SVC with a linear kernel, then SVC with an rbf kernel (all relatively close scores) and finally kNN was a good bit behind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a78148e-40f4-48da-a0b7-3b683407bed4",
   "metadata": {},
   "source": [
    "SVC likes things to be normalised, although the fact that these are images with values between 0 and 1 already means this may not really do much, and maybe with images, it should not even be attempted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13d5e8c7-cae2-4227-b0a4-f321e0671ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e0bc49-76f8-44ba-8f75-4b5c464af429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e8826-a2fb-4d87-ab2c-cac9f6e96347",
   "metadata": {},
   "source": [
    "A small improvement with StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3047eef-0c0c-42db-82b8-c7c58d41cb89",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "Now let's concentrate on SVC and pick the best one we can with that dataset. Import GridSearchCV and use that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfba0793-bb13-4032-974a-ed3bbc0eca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32868c9c-0cb9-4524-ab69-b7fafdf38fb4",
   "metadata": {},
   "source": [
    "You're going to want to try different values of C, different gammas and different kernels (linear, rbf, poly)\n",
    "\n",
    "While you can do them all in one param_grid, it might take longer as it will go through hyperparameters that have no affect on some kernels wasting time\n",
    "\n",
    "Look at the breast cancer example for how I approached it there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e499b9be-97c9-4887-a4bb-5ee4f456a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes after here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "394f274a-03cd-48a0-8632-a8a1e6f47f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "584695f0-0bb1-44c0-a74c-16829d484fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4bebb1-c689-4ffb-a1fa-a2b7821b5be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9cca3a-bb35-4e0e-8ae5-283d25dbd202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5022a27f-f63f-46b2-9216-e5135fc600aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1126057e-e24c-4c04-93f8-f24cd858ab85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb67160-4655-48d1-b951-66521b6e8970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8b5374-840a-4305-939e-0a24dec07bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be8f2f88-3c9c-410d-8fad-1629ad2a0f45",
   "metadata": {},
   "source": [
    "Record the best score you have had with all your cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34e524c-f2ff-4d57-8da9-af08741b2855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f4844-4cf9-4c0a-997b-391164d4b397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3b0420e-af78-4b0a-be89-7451f3288a3f",
   "metadata": {},
   "source": [
    "You could try it all using a StandardScaler but I think it might take a lot longer to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd174598-f315-4e40-8b3a-e2f3fc35f70e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735fc74a-5a9c-4379-bf3d-035c8f1a9b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65de97d5-1493-46fe-912d-18c9e9922bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee99ca6-f632-4bca-8d8f-c1391b182741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "560b5abb-b0a1-4d98-92d5-a733d8faf8aa",
   "metadata": {},
   "source": [
    "When you are done, take the best overall model parameters. Fit the model with the %time thing I did earlier. Same with scoring the model with the test set.\n",
    "\n",
    "Then do a classification report and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f1141-57b5-459a-b2d9-f891680ce594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdd8eef-3af7-43df-9db7-c8022f78faaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c0ad9f9-3a75-4e17-84f4-49387657a170",
   "metadata": {},
   "source": [
    "Tell a story about your results, explain what you think is going on and interpret the above reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2146b4c0-b1d3-4344-a9e2-f05e2ba78a92",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defdb80a-8055-4b00-9d6f-e56b6429323e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d1db729-578b-4f7a-b28a-b3a62a1a2906",
   "metadata": {},
   "source": [
    "Now try the whole thing all over again with\n",
    "\n",
    "                    faces = fetch_lfw_people(min_faces_per_person=60, resize=0.5)\n",
    "\n",
    "This will make the images smaller. Do you get the same or better results? It should be a lot faster training due to their being a lot less features. \n",
    "\n",
    "When doing train_test_split use the same random_state as you did previously, does it give you the same split then? \n",
    "\n",
    "Go through the same cross-validation to make your choices and do the same story with classification report/confusion matrix and timing of the fitting/scoring. This model should be faster, I don't know about the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd45088-18c9-4a3b-bf0a-5a74a0f1b3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a012e2a-c798-4394-a9a3-062f2d9f20ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa7791f-8283-4588-9996-0f1b29367cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953e191-d335-4564-af51-8f1f4f38b71f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a53d2-4d04-4274-803c-c56ec9263f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229cca7b-cb27-4e41-9243-fc0d77a3701c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
